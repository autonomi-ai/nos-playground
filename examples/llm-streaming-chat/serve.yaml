images:
  llm-py310-cu121:
    base: autonomi/nos:latest-py310-cu121
    pip:
      - git+https://github.com/casper-hansen/AutoAWQ
      - bitsandbytes

models:
  TheBloke/TinyLlama-1.1B-Chat-v1.0-AWQ:
    model_cls: StreamingChat
    model_path: models/chat.py
    init_kwargs:
      model_name: TheBloke/TinyLlama-1.1B-Chat-v1.0-AWQ
    default_method: chat
    runtime_env: llm-gpu
    deployment:
      resources:
        device: auto
        device_memory: 3Gi
